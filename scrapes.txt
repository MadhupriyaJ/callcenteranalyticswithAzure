
// call summary
// File upload endpoint

app.post('/upload', async (req, res) => {
  const audioFile = req.files.audio;
  const filePath = req.files.audio.path;

  console.log('Received file:', audioFile);
  console.log('filePath',filePath);
 


  if (!audioFile) {
    return res.status(400).json({ error: 'No file uploaded' });
  }
  try {
    // Perform transcription using speech API
    const transcription = await performSpeechToText(audioFile.data, audioFile.name);
    console.log("transcription", transcription);

    // Perform sentiment analysis
    const sentimentAnalysis = await performSentimentAnalysis([transcription]);
    console.log("sentimentanalysis",sentimentAnalysis);

  // Construct the data object containing transcription and sentiment analysis
  const data = {
    transcription: [
      {
        issue: "Issue summary",
        resolution: ["Resolution summary"]
      }
      // Add other summaries here if needed
    ],
    sentimentAnalysis: sentimentAnalysis,
    // Add other relevant data here if needed
  };


    // Save transcription to Azure Blob Storage
    const blobName = `${Date.now()}-${audioFile.originalname}`;
    const containerClient = blobServiceClient.getContainerClient('transcriptions');
    const blockBlobClient = containerClient.getBlockBlobClient(blobName);
    // await blockBlobClient.uploadFile(filePath);
  
    // Return the transcription and sentiment analysis results
    // res.json({ transcription, sentimentAnalysis, blobName });
  
  res.json({ transcription: [transcription], sentimentAnalysis, blobName });
  } catch (error) {

    console.error('Error uploading file:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});





// Speech to text function
const { Readable } = require('stream');
const performSpeechToText = async (audioFileData, originalFileName) => {
  console.log("audioFileData:", audioFileData);
  console.log("originalFileName:", originalFileName);

    // Check the type and size of audioFileData
  console.log("Type of audioFileData:", typeof audioFileData);
  console.log("Size of audioFileData:", audioFileData.length);

  

  // Save the audio file to a temporary location
  const filePath = path.join(__dirname, 'Uploads', originalFileName);
  console.log("filePath", filePath);

  fs.writeFileSync(filePath, audioFileData);


  // Check if the file exists after writing
  if (fs.existsSync(filePath)) {
    console.log('File saved successfully:', filePath);
  } else {
    console.log('Error saving file:', filePath);
    throw new Error('Error saving file');
  }

  const speechConfig = SpeechConfig.fromSubscription(SPEECH_API_SUBSCRIPTION_KEY, SPEECH_API_SERVICE_REGION);


  if (!speechConfig) {
    console.log('Failed to create speech config');
    throw new Error('Failed to create speech config');
  }

  // Convert audioFileData to a Buffer if it's an object
const audioBuffer = Buffer.isBuffer(audioFileData) ? audioFileData : Buffer.from(audioFileData);

// Create audio config using the audio Buffer
const audioConfig = AudioConfig.fromWavFileInput(audioBuffer);

  // // Create audio config using the file path
  // const audioConfig = AudioConfig.fromWavFileInput(filePath);

  if (!audioConfig) {
    console.log('Failed to create audio config');
    throw new Error('Failed to create audio config');
  }

  const recognizer = new SpeechRecognizer(speechConfig, audioConfig);
  let result = '';

  recognizer.recognizing = function (s, e) {
    console.log(`RECOGNIZING: ${e.result.text}`);
  };
  try {
    recognizer.recognized = function (s, e) {
      if (e.result.reason === ResultReason.RecognizedSpeech) {
        console.log(`RECOGNIZED: ${e.result.text}`);
        result += e.result.text + ' ';
      } else if (e.result.reason === ResultReason.NoMatch) {
        console.log('NOMATCH: Speech could not be recognized.');
      }
    };

    recognizer.canceled = function (s, e) {
      console.log(`CANCELED: Reason=${e.reason}`);
      recognizer.stopContinuousRecognition();
    };

    recognizer.sessionStopped = function (s, e) {
      console.log('\nSession stopped event.');
      recognizer.stopContinuousRecognition();
    };

    await recognizer.startContinuousRecognitionAsync();
    await new Promise(resolve => setTimeout(resolve, 60000)); // Adjust timeout as needed
    await recognizer.stopContinuousRecognitionAsync();

    return result;
  } catch (error) {
    console.error('Error performing speech to text:', error);
    throw error; // Re-throw the error for higher-level handling
  }
}


async function performSentimentAnalysis(text) {
  console.log("text",text);
  const sentimentAnalysisResult = await textAnalyticsClient.analyzeSentiment(text);
  // console.log("sentement analysis result",sentimentAnalysisResult);
  return sentimentAnalysisResult;
}
